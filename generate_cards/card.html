
<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <title>card</title>
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <section class="card-list">
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2505.01458" target="_blank" rel="noopener">
        <img src="../img/tab_img/A-Survey-of-Robotic-Navigation_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2505.01458" target="_blank" rel="noopener">A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI</a>
        </h4>
        <p class="cite-brief">Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Sim-to-Real Transfer</span><span class="tag">Physics Simulators</span><span class="tag">Embodied AI</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.01961" target="_blank" rel="noopener">
        <img src="../img/tab_img/AC-DiT_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.01961" target="_blank" rel="noopener">AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation</a>
        </h4>
        <p class="cite-brief">Recently, mobile manipulation has attracted increasing attention for enabling language-conditioned robotic control in household tasks. However, existing methods still face challenges in coordinating mobile base and manipulator, primarily due to two limitations. On the one hand, they fail to explicitly model the influence of the mobile base on manipulator control, which easily leads to error accumulation under high degrees of freedom. On the other hand, they treat the entire mobile manipulation process with the same visual observation modality (e.g., either all 2D or all 3D), overlooking the distinct multimodal perception requirements at different stages during mobile manipulation. To address this, we propose the Adaptive Coordination Diffusion Transformer (AC-DiT), which enhances mobile base and manipulator coordination for end-to-end mobile manipulation. First, since the motion of the mobile base directly influences the manipulator's actions, we introduce a mobility-to-body conditioning mechanism that guides the model to first extract base motion representations, which are then used as context prior for predicting whole-body actions. This enables whole-body control that accounts for the potential impact of the mobile base's motion. Second, to meet the perception requirements at different stages of mobile manipulation, we design a perception-aware multimodal conditioning strategy that dynamically adjusts the fusion weights between various 2D visual images and 3D point clouds, yielding visual features tailored to the current perceptual needs. This allows the model to, for example, adaptively rely more on 2D inputs when semantic information is crucial for action prediction, while placing greater emphasis on 3D geometric information when precise spatial understanding is required. We validate AC-DiT through extensive experiments on both simulated and real-world mobile manipulation tasks.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">AC-DiT</span><span class="tag">Mobile Manipulation</span><span class="tag">Multimodal Perception</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.sciencedirect.com/science/article/pii/S2949855425000516" target="_blank" rel="noopener">
        <img src="../img/tab_img/Agentic-AI_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.sciencedirect.com/science/article/pii/S2949855425000516" target="_blank" rel="noopener">Agentic AI: The age of reasoning—A review</a>
        </h4>
        <p class="cite-brief">Artificial intelligence has experienced a significant boom with the emergence of agentic AI, where autonomous agents are increasingly replacing human intervention, enabling systems to perceive, reason, and act independently to achieve specific goals. Despite its transformative potential, comprehensive information on agentic AI remains scarce in the literature. This paper provides the first comprehensive review of agentic AI, focusing on its evolution and three core aspects: patterns, types, and environments. The evolution of agentic AI is traced through five phases to the current era of multi-modal and collaborative agents, driven by advancements in reinforcement learning, neural networks, and large language models (LLMs). Five key patterns: tool use, reflection, ReAct, planning, and multi-agent collaboration (MAC) define how agentic AI systems interact and process tasks. These systems are categorized into seven categories, each tailored for specific operational styles and autonomy in decision making. The environments in which these agents operate are classified as static, dynamic, fully observable, partially observable, deterministic, stochastic, single-agent, and multi-agent, emphasizing the impact of environmental complexity on agent behavior. Agentic AI has revolutionized systems through autonomous decision making and resource optimization, yet challenges persist in aligning AI with human values, ensuring adaptability, and addressing ethical constraints. Future research focuses on multidomain agents, human-AI collaboration, and self-improving systems. This work provides researchers, practitioners, and policymakers with a structured approach to understanding and advancing the rapidly evolving landscape of agentic AI systems.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Agentic AI</span><span class="tag">Autonomous systems</span><span class="tag">LLMs</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2503.03081" target="_blank" rel="noopener">
        <img src="../img/tab_img/AirExo-2_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2503.03081" target="_blank" rel="noopener">AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons</a>
        </h4>
        <p class="cite-brief">Scaling up robotic imitation learning for real-world applications requires efficient and scalable demonstration collection methods. While teleoperation is effective, it depends on costly and inflexible robot platforms. In-the-wild demonstrations offer a promising alternative, but existing collection devices have key limitations: handheld setups offer limited observational coverage, and whole-body systems often require fine-tuning with robot data due to domain gaps. To address these challenges, we present AirExo-2, a low-cost exoskeleton system for large-scale in-the-wild data collection, along with several adaptors that transform collected data into pseudo-robot demonstrations suitable for policy learning. We further introduce RISE-2, a generalizable imitation learning policy that fuses 3D spatial and 2D semantic perception for robust manipulations. Experiments show that RISE-2 outperforms prior state-of-the-art methods on both in-domain and generalization evaluations. Trained solely on adapted in-the-wild data produced by AirExo-2, the RISE-2 policy achieves comparable performance to the policy trained with teleoperated data, highlighting the effectiveness and potential of AirExo-2 for scalable and generalizable imitation learning.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">AirExo-2</span><span class="tag">In-the-Wild Data Collection</span><span class="tag">Imitation Learning</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2506.19269" target="_blank" rel="noopener">
        <img src="../img/tab_img/AnchorDP3_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2506.19269" target="_blank" rel="noopener">AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic Manipulation</a>
        </h4>
        <p class="cite-brief">We present AnchorDP3, a diffusion policy framework for dual-arm robotic manipulation that achieves state-of-the-art performance in highly randomized environments. AnchorDP3 integrates three key innovations: (1) Simulator-Supervised Semantic Segmentation, using rendered ground truth to explicitly segment task-critical objects within the point cloud, which provides strong affordance priors; (2) Task-Conditioned Feature Encoders, lightweight modules processing augmented point clouds per task, enabling efficient multi-task learning through a shared diffusion-based action expert; (3) Affordance-Anchored Keypose Diffusion with Full State Supervision, replacing dense trajectory prediction with sparse, geometrically meaningful action anchors, i.e., keyposes such as pre-grasp pose, grasp pose directly anchored to affordances, drastically simplifying the prediction space; the action expert is forced to predict both robot joint angles and end-effector poses simultaneously, which exploits geometric consistency to accelerate convergence and boost accuracy. Trained on large-scale, procedurally generated simulation data, AnchorDP3 achieves a 98.7% average success rate in the RoboTwin benchmark across diverse tasks under extreme randomization of objects, clutter, table height, lighting, and backgrounds. This framework, when integrated with the RoboTwin real-to-sim pipeline, has the potential to enable fully autonomous generation of deployable visuomotor policies from only scene and instruction, totally eliminating human demonstrations from learning manipulation skills.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">AnchorDP3</span><span class="tag">Dual-Arm Robotic Manipulation</span><span class="tag">Affordance-Anchored Diffusion Policy</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.12768" target="_blank" rel="noopener">
        <img src="../img/tab_img/AnyPos_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.12768" target="_blank" rel="noopener">AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation</a>
        </h4>
        <p class="cite-brief">Vision-language-action (VLA) models have shown promise on task-conditioned control in complex settings such as bimanual manipulation. However, the heavy reliance on task-specific human demonstrations limits their generalization and incurs high data acquisition costs. In this work, we present a new notion of task-agnostic action paradigm that decouples action execution from task-specific conditioning, enhancing scalability, efficiency, and cost-effectiveness. To address the data collection challenges posed by this paradigm -- such as low coverage density, behavioral redundancy, and safety risks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a scalable self-supervised framework that accelerates collection by over  compared to human teleoperation. To further enable effective learning from task-agnostic data, which often suffers from distribution mismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics model equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder (DAD). We additionally integrate a video-conditioned action validation module to verify the feasibility of learned policies across diverse manipulation tasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51% improvement in test accuracy and achieves 30-40% higher success rates in downstream tasks such as lifting, pick-and-place, and clicking, using replay-based video validation.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">ATARA</span><span class="tag">Task-Agnostic Action Paradigm</span><span class="tag">AnyPos</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.02600" target="_blank" rel="noopener">
        <img src="../img/tab_img/ArtGS_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.02600" target="_blank" rel="noopener">ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects</a>
        </h4>
        <p class="cite-brief">Articulated object manipulation remains a critical challenge in robotics due to the complex kinematic constraints and the limited physical reasoning of existing methods. In this work, we introduce ArtGS, a novel framework that extends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling for articulated object understanding and interaction. ArtGS begins with multi-view RGB-D reconstruction, followed by reasoning with a vision-language model (VLM) to extract semantic and structural information, particularly the articulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS optimizes the parameters of the articulated bones, ensuring physically consistent motion constraints and enhancing the manipulation policy. By leveraging dynamic Gaussian splatting, cross-embodiment adaptability, and closed-loop optimization, ArtGS establishes a new framework for efficient, scalable, and generalizable articulated object modeling and manipulation. Experiments conducted in both simulation and real-world environments demonstrate that ArtGS significantly outperforms previous methods in joint estimation accuracy and manipulation success rates across a variety of articulated objects. </p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">ArtGS</span><span class="tag">Articulated Object Manipulation</span><span class="tag">3D Gaussian Splatting</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2505.14030" target="_blank" rel="noopener">
        <img src="../img/tab_img/AutoBio_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2505.14030" target="_blank" rel="noopener">AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory</a>
        </h4>
        <p class="cite-brief">Vision-language-action (VLA) models have shown promise as generalist robotic policies by jointly leveraging visual, linguistic, and proprioceptive modalities to generate action trajectories. While recent benchmarks have advanced VLA research in domestic tasks, professional science-oriented domains remain underexplored. We introduce AutoBio, a simulation framework and benchmark designed to evaluate robotic automation in biology laboratory environments--an application domain that combines structured protocols with demanding precision and multimodal interaction. AutoBio extends existing simulation capabilities through a pipeline for digitizing real-world laboratory instruments, specialized physics plugins for mechanisms ubiquitous in laboratory workflows, and a rendering stack that support dynamic instrument interfaces and transparent materials through physically based rendering. Our benchmark comprises biologically grounded tasks spanning three difficulty levels, enabling standardized evaluation of language-guided robotic manipulation in experimental protocols. We provide infrastructure for demonstration generation and seamless integration with VLA models. Baseline evaluations with two SOTA VLA models reveal significant gaps in precision manipulation, visual reasoning, and instruction following in scientific workflows. By releasing AutoBio, we aim to catalyze research on generalist robotic systems for complex, high-precision, and multimodal professional environments. The simulator and benchmark are publicly available to facilitate reproducible research.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">AutoBio</span><span class="tag">Vision-Language-Action Models</span><span class="tag">Biology Laboratory Automation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2506.23351" target="_blank" rel="noopener">
        <img src="../img/tab_img/Benchmarking-Generalizable-Bimanual-Manipulation_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2506.23351" target="_blank" rel="noopener">Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop</a>
        </h4>
        <p class="cite-brief">Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To advance this goal, we launched the RoboTwin Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot platform, the competition consisted of three stages: Simulation Round 1, Simulation Round 2, and a final Real-World Round. Participants totally tackled 17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based scenarios. The challenge attracted 64 global teams and over 400 participants, producing top-performing solutions like SEM and AnchorDP3 and generating valuable insights into generalizable bimanual policy learning. This report outlines the competition setup, task design, evaluation methodology, key findings and future direction, aiming to support future research on robust and generalizable bimanual manipulation policies. The Challenge Webpage is available at this https URL.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Embodied AI</span><span class="tag">Dual-Arm Manipulation</span><span class="tag">RoboTwin Challenge</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2506.06221" target="_blank" rel="noopener">
        <img src="../img/tab_img/BiAssemble_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2506.06221" target="_blank" rel="noopener">BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly</a>
        </h4>
        <p class="cite-brief">Shape assembly, the process of combining parts into a complete whole, is a crucial robotic skill with broad real-world applications. Among various assembly tasks, geometric assembly--where broken parts are reassembled into their original form (e.g., reconstructing a shattered bowl)--is particularly challenging. This requires the robot to recognize geometric cues for grasping, assembly, and subsequent bimanual collaborative manipulation on varied fragments. In this paper, we exploit the geometric generalization of point-level affordance, learning affordance aware of bimanual collaboration in geometric assembly with long-horizon action sequences. To address the evaluation ambiguity caused by geometry diversity of broken parts, we introduce a real-world benchmark featuring geometric variety and global reproducibility. Extensive experiments demonstrate the superiority of our approach over both previous affordance-based and imitation-based methods.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Geometric Assembly</span><span class="tag">Bimanual Collaboration</span><span class="tag">Point-Level Affordance</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://openreview.net/forum?id=3LvTtj0VYy" target="_blank" rel="noopener">
        <img src="../img/tab_img/ControlManip_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://openreview.net/forum?id=3LvTtj0VYy" target="_blank" rel="noopener">ControlManip: Few-Shot Manipulation Fine-tuning via Object-centric Conditional Control</a>
        </h4>
        <p class="cite-brief">Learning real-world robotic manipulation is challenging, particularly when limited demonstrations are available. Existing methods for few-shot manipulation often rely on simulation-augmented data or pre-built modules like grasping and pose estimation, which struggle with sim-to-real gaps and lack versatility. While large-scale imitation pre-training shows promise, adapting these general-purpose policies to specific tasks in data-scarce settings remains unexplored. To achieve this, we propose ControlManip, a novel framework that bridges pre-trained manipulation policies with object-centric representations via a ControlNet-style architecture for efficient fine-tuning. Specifically, to introduce object-centric conditions without overwriting prior knowledge, ControlManip zero-initializes a set of projection layers, allowing them to gradually adapt the pre-trained manipulation policies. In real-world experiments across 6 diverse tasks, including pouring cubes and folding clothes, our method achieves a 73.3% success rate while requiring only 10-20 demonstrations --- a significant improvement over traditional approaches that require more than 100 demonstrations to achieve comparable success. Comprehensive studies show that ControlManip improves the few-shot fine-tuning success rate by 252% over baselines and demonstrates robustness to object and background changes. By lowering the barriers to task development, ControlManip accelerates real-world robot adoption and lays the groundwork for unifying large-scale policy pre-training with object-centric representations.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">ControlManip</span><span class="tag">Few-Shot Robotic Manipulation</span><span class="tag">Object-Centric Representations</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2506.16211" target="_blank" rel="noopener">
        <img src="../img/tab_img/ControlVLA_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2506.16211" target="_blank" rel="noopener">ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models</a>
        </h4>
        <p class="cite-brief">Learning real-world robotic manipulation is challenging, particularly when limited demonstrations are available. Existing methods for few-shot manipulation often rely on simulation-augmented data or pre-built modules like grasping and pose estimation, which struggle with sim-to-real gaps and lack extensibility. While large-scale imitation pre-training shows promise, adapting these general-purpose policies to specific tasks in data-scarce settings remains unexplored. To achieve this, we propose ControlVLA, a novel framework that bridges pre-trained VLA models with object-centric representations via a ControlNet-style architecture for efficient fine-tuning. Specifically, to introduce object-centric conditions without overwriting prior knowledge, ControlVLA zero-initializes a set of projection layers, allowing them to gradually adapt the pre-trained manipulation policies. In real-world experiments across 6 diverse tasks, including pouring cubes and folding clothes, our method achieves a 76.7% success rate while requiring only 10-20 demonstrations -- a significant improvement over traditional approaches that require more than 100 demonstrations to achieve comparable success. Additional experiments highlight ControlVLA's extensibility to long-horizon tasks and robustness to unseen objects and backgrounds.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">ControlVLA</span><span class="tag">Few-Shot Manipulation</span><span class="tag">Object-Centric Representations</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2502.08449" target="_blank" rel="noopener">
        <img src="../img/tab_img/CordViP_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2502.08449" target="_blank" rel="noopener">CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World</a>
        </h4>
        <p class="cite-brief">Achieving human-level dexterity in robots is a key objective in the field of robotic manipulation. Recent advancements in 3D-based imitation learning have shown promising results, providing an effective pathway to achieve this goal. However, obtaining high-quality 3D representations presents two key problems: (1) the quality of point clouds captured by a single-view camera is significantly affected by factors such as camera resolution, positioning, and occlusions caused by the dexterous hand; (2) the global point clouds lack crucial contact information and spatial correspondences, which are necessary for fine-grained dexterous manipulation tasks. To eliminate these limitations, we propose CordViP, a novel framework that constructs and learns correspondences by leveraging the robust 6D pose estimation of objects and robot proprioception. Specifically, we first introduce the interaction-aware point clouds, which establish correspondences between the object and the hand. These point clouds are then used for our pre-training policy, where we also incorporate object-centric contact maps and hand-arm coordination information, effectively capturing both spatial and temporal dynamics. Our method demonstrates exceptional dexterous manipulation capabilities, achieving state-of-the-art performance in six real-world tasks, surpassing other baselines by a large margin. Experimental results also highlight the superior generalization and robustness of CordViP to different objects, viewpoints, and scenarios. Code and videos are available on this https URL.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">CordViP</span><span class="tag">Dexterous Manipulation</span><span class="tag">Interaction-Aware Point Clouds</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.mdpi.com/2673-4052/6/3/41" target="_blank" rel="noopener">
        <img src="../img/tab_img/DISCOVERSE_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.mdpi.com/2673-4052/6/3/41" target="_blank" rel="noopener">Design and Development of Cost-Effective Humanoid Robots for Enhanced Human–Robot Interaction</a>
        </h4>
        <p class="cite-brief">Industry Revolution Five (Industry 5.0) will shift the focus away from technology and rely more on to the collaboration between humans and AI-powered robots. This approach emphasizes a more human-centric perspective, enhanced resilience, optimized workplace processes, and a stronger commitment to sustainability. The humanoid robot market has experienced substantial growth, fueled by technological advancements and the increasing need for automation in industries such as service, customer support, and education. However, challenges like high costs, complex maintenance, and societal concerns about job displacement remain. Despite these issues, the market is expected to continue expanding, supported by innovations that enhance both accessibility and performance. Therefore, this article proposes the design and implementation of low-cost, remotely controlled humanoid robots via a mobile application for home-assistant applications. The humanoid robot boasts an advanced mechanical structure, high-performance actuators, and an array of sensors that empower it to execute a wide range of tasks with human-like dexterity and mobility. Incorporating sophisticated control algorithms and a user-friendly Graphical User Interface (GUI) provides precise and stable robot operation and control. Through an in-house developed code, our research contributes to the growing field of humanoid robotics and underscores the significance of advanced control systems in fully harnessing the capabilities of these human-like machines. The implications of our findings extend to the future development and deployment of humanoid robots across various industries and societal contexts, making this an ideal area for students and researchers to explore innovative solutions.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Industry 5.0</span><span class="tag">Humanoid Robots</span><span class="tag">Human-Centric Automation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://openaccess.thecvf.com/content/CVPR2025/html/Liang_DexHandDiff_Interaction-aware_Diffusion_Planning_for_Adaptive_Dexterous_Manipulation_CVPR_2025_paper.html" target="_blank" rel="noopener">
        <img src="../img/tab_img/Design-and-Development_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Liang_DexHandDiff_Interaction-aware_Diffusion_Planning_for_Adaptive_Dexterous_Manipulation_CVPR_2025_paper.html" target="_blank" rel="noopener">DexHandDiff: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation</a>
        </h4>
        <p class="cite-brief">Dexterous manipulation with contact-rich interactions is crucial for advanced robotics. While recent diffusion-based planning approaches show promise for simple manipulation tasks, they often produce unrealistic ghost states (e.g., the object automatically moves without hand contact) or lack adaptability when handling complex sequential interactions. In this work, we introduce DexHandDiff, an interaction-aware diffusion planning framework for adaptive dexterous manipulation. DexHandDiff models joint state-action dynamics through a dual-phase diffusion process which consists of pre-interaction contact alignment and post-contact goal-directed control, enabling goal-adaptive generalizable dexterous manipulation. Additionally, we incorporate dynamics model-based dual guidance and leverage large language models for automated guidance function generation, enhancing generalizability for physical interactions and facilitating diverse goal adaptation through language cues. Experiments on physical interaction tasks such as door opening, pen and block re-orientation, object relocation, and hammer striking demonstrate DexHandDiff's effectiveness on goals outside training distributions, achieving over twice the average success rate (59.2% vs. 29.5%) compared to existing methods. Our framework achieves an average of 70.7% success rate on goal adaptive dexterous tasks, highlighting its robustness and flexibility in contact-rich manipulation.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">DexHandDiff</span><span class="tag">Dexterous Manipulation</span><span class="tag">Diffusion Planning Framework</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.11296" target="_blank" rel="noopener">
        <img src="../img/tab_img/DexHandDiff_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.11296" target="_blank" rel="noopener">Diffusion-Based Imaginative Coordination for Bimanual Manipulation</a>
        </h4>
        <p class="cite-brief">Bimanual manipulation is crucial in robotics, enabling complex tasks in industrial automation and household services. However, it poses significant challenges due to the high-dimensional action space and intricate coordination requirements. While video prediction has been recently studied for representation learning and control, leveraging its ability to capture rich dynamic and behavioral information, its potential for enhancing bimanual coordination remains underexplored. To bridge this gap, we propose a unified diffusion-based framework for the joint optimization of video and action prediction. Specifically, we propose a multi-frame latent prediction strategy that encodes future states in a compressed latent space, preserving task-relevant features. Furthermore, we introduce a unidirectional attention mechanism where video prediction is conditioned on the action, while action prediction remains independent of video prediction. This design allows us to omit video prediction during inference, significantly enhancing efficiency. Experiments on two simulated benchmarks and a real-world setting demonstrate a significant improvement in the success rate over the strong baseline ACT using our method, achieving a /textbf{24.9/%} increase on ALOHA, an /textbf{11.1/%} increase on RoboTwin, and a /textbf{32.5/%} increase in real-world experiments. </p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Bimanual Manipulation</span><span class="tag">Diffusion-Based Framework</span><span class="tag">Video-Action Prediction</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://ieeexplore.ieee.org/abstract/document/11030084/" target="_blank" rel="noopener">
        <img src="../img/tab_img/Diffusion-Based-Imaginative-Coordination_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://ieeexplore.ieee.org/abstract/document/11030084/" target="_blank" rel="noopener">Digital Twin for Robotic Vision: A Blender-Based Approach</a>
        </h4>
        <p class="cite-brief">Digital twins are widely used alongside deep learning techniques for robotic vision, offering a safe and cost-effective means for model training and data collection. However, most existing approaches utilize digital twins solely for offline synthetic data generation, which is typically sufficient for training object detection models but may fall short for more complex tasks such as navigation, path planning, obstacle avoidance, and grasping. Additionally, few methods take real-world camera parameters into account, leading to misalignment between synthetic images and real-world camera views. A key challenge in training deep learning models with synthetic data is sim-to-real transfer. Ensuring precise alignment between synthetic and real images not only enhances the realism of synthetic data but also enables automatic annotation of real-world images, further improving sim-to-real performance. In this paper, we present a Blenderbased digital twin for deep learning-driven robotic vision. Our approach supports real-time updates to the virtual environment and incorporates real-world camera parameters, facilitating the creation of datasets that enhance sim-to-real transfer for deep learning models.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Digital Twin</span><span class="tag">Robotic Vision</span><span class="tag">Sim-to-Real Transfer</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.21981" target="_blank" rel="noopener">
        <img src="../img/tab_img/Digital-Twin-for-Robotic-Vision_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.21981" target="_blank" rel="noopener">DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity Environments</a>
        </h4>
        <p class="cite-brief">We present the first unified, modular, open-source 3DGS-based simulation framework for Real2Sim2Real robot learning. It features a holistic Real2Sim pipeline that synthesizes hyper-realistic geometry and appearance of complex real-world scenarios, paving the way for analyzing and bridging the Sim2Real gap. Powered by Gaussian Splatting and MuJoCo, Discoverse enables massively parallel simulation of multiple sensor modalities and accurate physics, with inclusive supports for existing 3D assets, robot models, and ROS plugins, empowering large-scale robot learning and complex robotic benchmarks. Through extensive experiments on imitation learning, Discoverse demonstrates state-of-the-art zero-shot Sim2Real transfer performance compared to existing simulators. For code and demos: this https URL.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Discoverse</span><span class="tag">Real2Sim2Real</span><span class="tag">3D Gaussian Splatting</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.tandfonline.com/doi/abs/10.1080/00207543.2025.2499866" target="_blank" rel="noopener">
        <img src="../img/tab_img/Distributed-multi-robot-task_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.tandfonline.com/doi/abs/10.1080/00207543.2025.2499866" target="_blank" rel="noopener">Distributed multi-robot task dynamic allocation in digital-twin factory towards industry 5.0</a>
        </h4>
        <p class="cite-brief">Task allocation is a critical component in multi-robot manufacturing systems (MRMS), significantly affecting operational efficiency and system resilience. Within the framework of industry 5.0, which emphasises decentralised autonomous manufacturing and sustainability, traditional static and centralised task allocation methods prove inadequate for the dynamic and complex demands of contemporary manufacturing environments. To overcome these limitations, this study introduces a distributed multi-robot task dynamic allocation method for digital twin factories. We develop a digital twin-driven robot model, coupled with the task dynamic allocation process to facilitate real-time monitoring and anomaly resolution. To derive high-quality and scalable solutions within a decentralised framework, the Alternating Direction Method of Multipliers (ADMM) and Augmented Lagrangian Coordination (ALC) methods are employed to analyze and resolve task allocation challenges. The proposed methodology enhances system configurability and resilience, aligning with the sustainability objectives of industry 5.0. The efficacy and efficiency of the proposed method are demonstrated through a practical case study.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Task Allocation</span><span class="tag">Multi-Robot Manufacturing Systems</span><span class="tag">Industry 5.0</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2505.06678" target="_blank" rel="noopener">
        <img src="../img/tab_img/Distributionally-Robust-Contract-Theory_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2505.06678" target="_blank" rel="noopener">Distributionally Robust Contract Theory for Edge AIGC Services in Teleoperation</a>
        </h4>
        <p class="cite-brief">Advanced AI-Generated Content (AIGC) technologies have injected new impetus into teleoperation, further enhancing its security and efficiency. Edge AIGC networks have been introduced to meet the stringent low-latency requirements of teleoperation. However, the inherent uncertainty of AIGC service quality and the need to incentivize AIGC service providers (ASPs) make the design of a robust incentive mechanism essential. This design is particularly challenging due to both uncertainty and information asymmetry, as teleoperators have limited knowledge of the remaining resource capacities of ASPs. To this end, we propose a distributionally robust optimization (DRO)-based contract theory to design robust reward schemes for AIGC task offloading. Notably, our work extends the contract theory by integrating DRO, addressing the fundamental challenge of contract design under uncertainty. In this paper, contract theory is employed to model the information asymmetry, while DRO is utilized to capture the uncertainty in AIGC service quality. Given the inherent complexity of the original DRO-based contract theory problem, we reformulate it into an equivalent, tractable bi-level optimization problem. To efficiently solve this problem, we develop a Block Coordinate Descent (BCD)-based algorithm to derive robust reward schemes. Simulation results on our unity-based teleoperation platform demonstrate that the proposed method improves teleoperator utility by 2.7/% to 10.74/% under varying degrees of AIGC service quality shifts and increases ASP utility by 60.02/% compared to the SOTA method, i.e., Deep Reinforcement Learning (DRL)-based contract theory. </p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">AIGC Teleoperation</span><span class="tag">Distributionally Robust Optimization</span><span class="tag">Contract Theory</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://openaccess.thecvf.com/content/CVPR2025W/MEIS/html/Yu_Efficient_Task-specific_Conditional_Diffusion_Policies_Shortcut_Model_Acceleration_and_SO3_CVPRW_2025_paper.html" target="_blank" rel="noopener">
        <img src="../img/tab_img/ERMV_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://openaccess.thecvf.com/content/CVPR2025W/MEIS/html/Yu_Efficient_Task-specific_Conditional_Diffusion_Policies_Shortcut_Model_Acceleration_and_SO3_CVPRW_2025_paper.html" target="_blank" rel="noopener">Efficient Task-specific Conditional Diffusion Policies: Shortcut Model Acceleration and SO(3) Optimization</a>
        </h4>
        <p class="cite-brief">Imitation learning, particularly Diffusion Policies based methods, has recently gained significant traction in embod- ied AI as a powerful approach to action policy generation. These models efficiently generate action policies by learn- ing to predict noise. However, conventional Diffusion Pol- icy methods rely on iterative denoising, leading to ineffi- cient inference and slow response times, which hinder real- time robot control. To address these limitations, we pro- pose a Classifier-Free Shortcut Diffusion Policy (CF-SDP) that integrates classifier-free guidance with shortcut-based acceleration, enabling efficient task-specific action gener- ation while significantly improving inference speed. Fur- thermore, we extend diffusion modeling to the SO(3) man- ifold in shortcut model, defining the forward and reverse processes in its tangent space with an isotropic Gaussian distribution. This ensures stable and accurate rotational estimation, enhancing the effectiveness of diffusion-based control. Our approach achieves nearly 5x acceleration in diffusion inference compared to DDIM-based Diffusion Pol- icy while maintaining task performance. Evaluations both on the RoboTwin simulation platform and real-world sce- narios across various tasks demonstrate the superiority of our method.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Classifier-Free Shortcut Diffusion Policy</span><span class="tag">Efficient Inference</span><span class="tag">SO(3) Manifold</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.05198" target="_blank" rel="noopener">
        <img src="../img/tab_img/Efficient-Task-specific-Conditional-Diffusion-Policies_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.05198" target="_blank" rel="noopener">EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling</a>
        </h4>
        <p class="cite-brief">The rapid advancement of Embodied AI has led to an increasing demand for large-scale, high-quality real-world data. However, collecting such embodied data remains costly and inefficient. As a result, simulation environments have become a crucial surrogate for training robot policies. Yet, the significant Real2Sim2Real gap remains a critical bottleneck, particularly in terms of physical dynamics and visual appearance. To address this challenge, we propose EmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both the physics and appearance perspectives. Specifically, we propose PhysAligner, a differentiable physics module designed to reduce the Real2Sim physical gap. It jointly optimizes robot-specific parameters such as control gains and friction coefficients to better align simulated dynamics with real-world observations. In addition, we introduce VisAligner, which incorporates a conditional video diffusion model to bridge the Sim2Real appearance gap by translating low-fidelity simulated renderings into photorealistic videos conditioned on simulation states, enabling high-fidelity visual transfer. Extensive experiments validate the effectiveness of EmbodieDreamer. The proposed PhysAligner reduces physical parameter estimation error by 3.74% compared to simulated annealing methods while improving optimization speed by 89.91/%. Moreover, training robot policies in the generated photorealistic environment leads to a 29.17% improvement in the average task success rate across real-world tasks after reinforcement learning. Code, model and data will be publicly available.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">EmbodieDreamer</span><span class="tag">Real2Sim2Real Gap</span><span class="tag">Differentiable Physics and Visual Alignment</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.17462" target="_blank" rel="noopener">
        <img src="../img/tab_img/EmbodieDreamer_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.17462" target="_blank" rel="noopener">ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents</a>
        </h4>
        <p class="cite-brief">Robot imitation learning relies on 4D multi-view sequential images. However, the high cost of data collection and the scarcity of high-quality data severely constrain the generalization and application of embodied intelligence policies like Vision-Language-Action (VLA) models. Data augmentation is a powerful strategy to overcome data scarcity, but methods for editing 4D multi-view sequential images for manipulation tasks are currently lacking. Thus, we propose ERMV (Editing Robotic Multi-View 4D data), a novel data augmentation framework that efficiently edits an entire multi-view sequence based on single-frame editing and robot state conditions. This task presents three core challenges: (1) maintaining geometric and appearance consistency across dynamic views and long time horizons; (2) expanding the working window with low computational costs; and (3) ensuring the semantic integrity of critical objects like the robot arm. ERMV addresses these challenges through a series of innovations. First, to ensure spatio-temporal consistency in motion blur, we introduce a novel Epipolar Motion-Aware Attention (EMA-Attn) mechanism that learns pixel shift caused by movement before applying geometric constraints. Second, to maximize the editing working window, ERMV pioneers a Sparse Spatio-Temporal (STT) module, which decouples the temporal and spatial views and remodels a single-frame multi-view problem through sparse sampling of the views to reduce computational demands. Third, to alleviate error accumulation, we incorporate a feedback intervention Mechanism, which uses a Multimodal Large Language Model (MLLM) to check editing inconsistencies and request targeted expert guidance only when necessary. Extensive experiments demonstrate that ERMV-augmented data significantly boosts the robustness and generalization of VLA models in both simulated and real-world environments.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">ERMV</span><span class="tag">Data Augmentation</span><span class="tag">Multi-View 4D Robot Imitation Learning</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_G3Flow_Generative_3D_Semantic_Flow_for_Pose-aware_and_Generalizable_Object_CVPR_2025_paper.html" target="_blank" rel="noopener">
        <img src="../img/tab_img/G3Flow_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Chen_G3Flow_Generative_3D_Semantic_Flow_for_Pose-aware_and_Generalizable_Object_CVPR_2025_paper.html" target="_blank" rel="noopener">G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation</a>
        </h4>
        <p class="cite-brief">Recent advances in imitation learning for 3D robotic manipulation have shown promising results with diffusion-based policies. However, achieving human-level dexterity requires seamless integration of geometric precision and semantic understanding. We present G3Flow, a novel framework that constructs real-time semantic flow, a dynamic, object-centric 3D semantic representation by leveraging foundation models. Our approach uniquely combines 3D generative models for digital twin creation, vision foundation models for semantic feature extraction, and robust pose tracking for continuous semantic flow updates. This integration enables complete semantic understanding even under occlusions while eliminating manual annotation requirements. By incorporating semantic flow into diffusion policies, we demonstrate significant improvements in both terminal-constrained manipulation and cross-object generalization. Extensive experiments across five simulation tasks show that G3Flow consistently outperforms existing approaches, achieving up to 68.3% and 50.1% average success rates on terminal-constrained manipulation and cross-object generalization tasks respectively. Our results demonstrate the effectiveness of G3Flow in enhancing real-time dynamic semantic feature understanding for robotic manipulation policies.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Semantic Flow</span><span class="tag">Diffusion Policies</span><span class="tag">3D Robotic Manipulation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://search.proquest.com/openview/838ae5ea5f18cc1e92bd879b3aaf2429/1?pq-origsite=gscholar&cbl=18750&diss=y" target="_blank" rel="noopener">
        <img src="../img/tab_img/GSWorld_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://search.proquest.com/openview/838ae5ea5f18cc1e92bd879b3aaf2429/1?pq-origsite=gscholar&cbl=18750&diss=y" target="_blank" rel="noopener">GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation</a>
        </h4>
        <p class="cite-brief">This paper builds GSWorld, a robust photo-realistic simulator for robotics manipulation by combining 3D Gaussian Splatting with physics engines. Our framework advocates `closing the loop' of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and automated photorealistic data collection without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF containing 3 robot embodiments for single-arm and bimanual manipulation as well as 29 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting iii applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering (2) reproducible benchmarking of real-robot manipulation policies in simulation, and (3) automated high-quality DAGGER data collection for adapting policies to deployment environments. We plan to open-source both the GSDF assets and code</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">GSWorld</span><span class="tag">3D Gaussian Splatting</span><span class="tag">Photo-Realistic Simulation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2508.17600" target="_blank" rel="noopener">
        <img src="../img/tab_img/GWM_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2508.17600" target="_blank" rel="noopener">GWM: Towards Scalable Gaussian World Models for Robotic Manipulation</a>
        </h4>
        <p class="cite-brief">Training robot policies within a learned world model is trending due to the inefficiency of real-world interactions. The established image-based world models and policies have shown prior success, but lack robust geometric information that requires consistent spatial and physical understanding of the three-dimensional world, even pre-trained on internet-scale video sources. To this end, we propose a novel branch of world model named Gaussian World Model (GWM) for robotic manipulation, which reconstructs the future state by inferring the propagation of Gaussian primitives under the effect of robot actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction with Gaussian Splatting. GWM can not only enhance the visual representation for imitation learning agent by self-supervised future prediction training, but can serve as a neural simulator that supports model-based reinforcement learning. Both simulated and real-world experiments depict that GWM can precisely predict future scenes conditioned on diverse robot actions, and can be further utilized to train policies that outperform the state-of-the-art by impressive margins, showcasing the initial data scaling potential of 3D world model.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Gaussian World Model</span><span class="tag">Diffusion Transformer</span><span class="tag">Robotic Manipulation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.23523" target="_blank" rel="noopener">
        <img src="../img/tab_img/H-RDT_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.23523" target="_blank" rel="noopener">H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</a>
        </h4>
        <p class="cite-brief">Imitation learning for robotic manipulation faces a fundamental challenge: the scarcity of large-scale, high-quality robot demonstration data. Recent robotic foundation models often pre-train on cross-embodiment robot datasets to increase data scale, while they face significant limitations as the diverse morphologies and action spaces across different robot embodiments make unified training challenging. In this paper, we present H-RDT (Human to Robotics Diffusion Transformer), a novel approach that leverages human manipulation data to enhance robot manipulation capabilities. Our key insight is that large-scale egocentric human manipulation videos with paired 3D hand pose annotations provide rich behavioral priors that capture natural manipulation strategies and can benefit robotic policy learning. We introduce a two-stage training paradigm: (1) pre-training on large-scale egocentric human manipulation data, and (2) cross-embodiment fine-tuning on robot-specific data with modular action encoders and decoders. Built on a diffusion transformer architecture with 2B parameters, H-RDT uses flow matching to model complex action distributions. Extensive evaluations encompassing both simulation and real-world experiments, single-task and multitask scenarios, as well as few-shot learning and robustness assessments, demonstrate that H-RDT outperforms training from scratch and existing state-of-the-art methods, including Pi0 and RDT, achieving significant improvements of 13.9% and 40.5% over training from scratch in simulation and real-world experiments, respectively. The results validate our core hypothesis that human manipulation data can serve as a powerful foundation for learning bimanual robotic manipulation policies.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Imitation Learning</span><span class="tag">H-RDT</span><span class="tag">Human-to-Robot</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2503.00508" target="_blank" rel="noopener">
        <img src="../img/tab_img/HGDiffuser_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2503.00508" target="_blank" rel="noopener">HGDiffuser: Efficient Task-Oriented Grasp Generation via Human-Guided Grasp Diffusion Models</a>
        </h4>
        <p class="cite-brief">HGDiffuser; Task-Oriented Grasping; Diffusion Transformer</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">HGDiffuser</span><span class="tag">Task-Oriented Grasping</span><span class="tag">Diffusion-Based Framework</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.00833" target="_blank" rel="noopener">
        <img src="../img/tab_img/HumanoidGen_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.00833" target="_blank" rel="noopener">HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning</a>
        </h4>
        <p class="cite-brief">For robotic manipulation, existing robotics datasets and simulation benchmarks predominantly cater to robot-arm platforms. However, for humanoid robots equipped with dual arms and dexterous hands, simulation tasks and high-quality demonstrations are notably lacking. Bimanual dexterous manipulation is inherently more complex, as it requires coordinated arm movements and hand operations, making autonomous data collection challenging. This paper presents HumanoidGen, an automated task creation and demonstration collection framework that leverages atomic dexterous operations and LLM reasoning to generate relational constraints. Specifically, we provide spatial annotations for both assets and dexterous hands based on the atomic operations, and perform an LLM planner to generate a chain of actionable spatial constraints for arm movements based on object affordances and scenes. To further improve planning ability, we employ a variant of Monte Carlo tree search to enhance LLM reasoning for long-horizon tasks and insufficient annotation. In experiments, we create a novel benchmark with augmented scenarios to evaluate the quality of the collected data. The results show that the performance of the 2D and 3D diffusion policies can scale with the generated dataset. </p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">HumanoidGen</span><span class="tag">Bimanual Dexterous Manipulation</span><span class="tag">Automated Demonstration Collection</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2503.13171" target="_blank" rel="noopener">
        <img src="../img/tab_img/HyCodePolicy_img copy.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2503.13171" target="_blank" rel="noopener">HybridGen: VLM-Guided Hybrid Planning for Scalable Data Generation of Imitation Learning</a>
        </h4>
        <p class="cite-brief">The acquisition of large-scale and diverse demonstration data are essential for improving robotic imitation learning generalization. However, generating such data for complex manipulations is challenging in real-world settings. We introduce HybridGen, an automated framework that integrates Vision-Language Model (VLM) and hybrid planning. HybridGen uses a two-stage pipeline: first, VLM to parse expert demonstrations, decomposing tasks into expert-dependent (object-centric pose transformations for precise control) and plannable segments (synthesizing diverse trajectories via path planning); second, pose transformations substantially expand the first-stage data. Crucially, HybridGen generates a large volume of training data without requiring specific data formats, making it broadly applicable to a wide range of imitation learning algorithms, a characteristic which we also demonstrate empirically across multiple algorithms. Evaluations across seven tasks and their variants demonstrate that agents trained with HybridGen achieve substantial performance and generalization gains, averaging a 5% improvement over state-of-the-art methods. Notably, in the most challenging task variants, HybridGen achieves significant improvement, reaching a 59.7% average success rate, significantly outperforming Mimicgen's 49.5%. These results demonstrating its effectiveness and practicality.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">HybridGen</span><span class="tag">Imitation Learning</span><span class="tag">Vision-Language Model</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2508.02629" target="_blank" rel="noopener">
        <img src="../img/tab_img/HyCodePolicy_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2508.02629" target="_blank" rel="noopener">HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents</a>
        </h4>
        <p class="cite-brief">Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">HyCodePolicy</span><span class="tag">Code Policy Generation</span><span class="tag">Embodied Agents</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2508.02629" target="_blank" rel="noopener">
        <img src="../img/tab_img/HybridGen_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2508.02629" target="_blank" rel="noopener">HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents</a>
        </h4>
        <p class="cite-brief">Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">HyCodePolicy</span><span class="tag">Code Synthesis and Repair</span><span class="tag">Embodied Agents</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2508.02629" target="_blank" rel="noopener">
        <img src="../img/tab_img/Information-Theoretic-Graph-Fusion.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2508.02629" target="_blank" rel="noopener">HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents</a>
        </h4>
        <p class="cite-brief">Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">HyCodePolicy</span><span class="tag">Code Policy Generation</span><span class="tag">Perceptual Monitoring and Repair</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2508.05342" target="_blank" rel="noopener">
        <img src="../img/tab_img/LLM-driven-Indoor_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2508.05342" target="_blank" rel="noopener">Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control</a>
        </h4>
        <p class="cite-brief">Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">GF-VLA</span><span class="tag">Graph-Based Scene Representation</span><span class="tag">Dual-Arm Robotic Manipulation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.14809" target="_blank" rel="noopener">
        <img src="../img/tab_img/Light-Future_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.14809" target="_blank" rel="noopener">Light Future: Multimodal Action Frame Prediction via InstructPix2Pix</a>
        </h4>
        <p class="cite-brief">Predicting future motion trajectories is a critical capability across domains such as robotics, autonomous systems, and human activity forecasting, enabling safer and more intelligent decision-making. This paper proposes a novel, efficient, and lightweight approach for robot action prediction, offering significantly reduced computational cost and inference latency compared to conventional video prediction models. Importantly, it pioneers the adaptation of the InstructPix2Pix model for forecasting future visual frames in robotic tasks, extending its utility beyond static image editing. We implement a deep learning-based visual prediction framework that forecasts what a robot will observe 100 frames (10 seconds) into the future, given a current image and a textual instruction. We repurpose and fine-tune the InstructPix2Pix model to accept both visual and textual inputs, enabling multimodal future frame prediction. Experiments on the RoboTWin dataset (generated based on real-world scenarios) demonstrate that our method achieves superior SSIM and PSNR compared to state-of-the-art baselines in robot action prediction tasks. Unlike conventional video prediction models that require multiple input frames, heavy computation, and slow inference latency, our approach only needs a single image and a text prompt as input. This lightweight design enables faster inference, reduced GPU demands, and flexible multimodal control, particularly valuable for applications like robotics and sports motion trajectory analytics, where motion trajectory precision is prioritized over visual fidelity.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">InstructPix2Pix</span><span class="tag">Robot Action Prediction</span><span class="tag">Future Frame Forecasting</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.researchgate.net/profile/Georgios-Drakopoulos/publication/393671925_Linear_Algebraic_Properties_Of_Quantum_Gates_As_A_Starting_Point_For_Their_Digital_Twinning/links/687538eedd6b84447df876f2/Linear-Algebraic-Properties-Of-Quantum-Gates-As-A-Starting-Point-For-Their-Digital-Twinning.pdf" target="_blank" rel="noopener">
        <img src="../img/tab_img/Linear-Algebraic-Properties_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.researchgate.net/profile/Georgios-Drakopoulos/publication/393671925_Linear_Algebraic_Properties_Of_Quantum_Gates_As_A_Starting_Point_For_Their_Digital_Twinning/links/687538eedd6b84447df876f2/Linear-Algebraic-Properties-Of-Quantum-Gates-As-A-Starting-Point-For-Their-Digital-Twinning.pdf" target="_blank" rel="noopener">Linear Algebraic Properties Of Quantum Gates As A Starting Point For Their Digital Twinning</a>
        </h4>
        <p class="cite-brief">Quantum circuits are one of the three universal quantum computing paradigms and by far the most popular and intuitive one, the two other being quantum walks and adiabatic quantum computing. Ad_x0002_ditionally, quantum circuits can verify the validity of computations done in special purpose paradigms like quantum annealing. From an architec_x0002_tural perspective the most important components of quantum circuits are quantum gates and the connectivity between them. Because of the physical properties and the limitations of existing implementations, it is paramount that quantum gate operation be digitally twinned in classical computers for verification as well as study and observation purposes. A starting point is the detailed description of their linear algebraic prop_x0002_erties since each quantum gate corresponds to a unitary operator on a suitably defined Hilbert space. Said properties include the matrix form, the eigenvalues, the determinant, the trace, the effect of basis qubits, and their geometric interpretations, as the latter is fundamental in un_x0002_derstanding quantum computing. Additionally, it is explored how these properties can be directly translated to proper Pythonic code.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Quantum Circuits</span><span class="tag">Quantum Gates</span><span class="tag">Digital Twin Verification</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2506.07570" target="_blank" rel="noopener">
        <img src="../img/tab_img/ManiFlow_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2506.07570" target="_blank" rel="noopener">LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization</a>
        </h4>
        <p class="cite-brief">Automatic indoor layout generation has attracted increasing attention due to its potential in interior design, virtual environment construction, and embodied AI. Existing methods fall into two categories: prompt-driven approaches that leverage proprietary LLM services (e.g., GPT APIs) and learning-based methods trained on layout data upon diffusion-based models. Prompt-driven methods often suffer from spatial inconsistency and high computational costs, while learning-based methods are typically constrained by coarse relational graphs and limited datasets, restricting their generalization to diverse room categories. In this paper, we revisit LLM-based indoor layout generation and present 3D-SynthPlace, a large-scale dataset that combines synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline, upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000 scenes, covering four common room types -- bedroom, living room, kitchen, and bathroom -- enriched with diverse objects and high-level spatial annotations. We further introduce OptiScene, a strong open-source LLM optimized for indoor layout generation, fine-tuned based on our 3D-SynthPlace dataset through our two-stage training. For the warum-up stage I, we adopt supervised fine-tuning (SFT), which is taught to first generate high-level spatial descriptions then conditionally predict concrete object placements. For the reinforcing stage II, to better align the generated layouts with human design preferences, we apply multi-turn direct preference optimization (DPO), which significantly improving layout quality and generation success rates. Extensive experiments demonstrate that OptiScene outperforms traditional prompt-driven and learning-based baselines. Moreover, OptiScene shows promising potential in interactive tasks such as scene editing and robot navigation.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">3D-SynthPlace</span><span class="tag">OptiScene</span><span class="tag">Indoor Layout Generation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2509.01819" target="_blank" rel="noopener">
        <img src="../img/tab_img/Modality-Composable-Diffusion-Policy_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2509.01819" target="_blank" rel="noopener">ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training</a>
        </h4>
        <p class="cite-brief">This paper introduces ManiFlow, a visuomotor imitation learning policy for general robot manipulation that generates precise, high-dimensional actions conditioned on diverse visual, language and proprioceptive inputs. We leverage flow matching with consistency training to enable high-quality dexterous action generation in just 1-2 inference steps. To handle diverse input modalities efficiently, we propose DiT-X, a diffusion transformer architecture with adaptive cross-attention and AdaLN-Zero conditioning that enables fine-grained feature interactions between action tokens and multi-modal observations. ManiFlow demonstrates consistent improvements across diverse simulation benchmarks and nearly doubles success rates on real-world tasks across single-arm, bimanual, and humanoid robot setups with increasing dexterity. The extensive evaluation further demonstrates the strong robustness and generalizability of ManiFlow to novel objects and background changes, and highlights its strong scaling capability with larger-scale datasets. Our website: this http URL.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">ManiFlow</span><span class="tag">Imitation Learning</span><span class="tag">Visuomotor Policy</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2503.12466" target="_blank" rel="noopener">
        <img src="../img/tab_img/Multi-robot-collaborative-manufacturing-driven-by-digital-twins_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2503.12466" target="_blank" rel="noopener">Modality-Composable Diffusion Policy via Inference-Time Distribution-level Composition</a>
        </h4>
        <p class="cite-brief">Diffusion Policy (DP) has attracted significant attention as an effective method for policy representation due to its capacity to model multi-distribution dynamics. However, current DPs are often based on a single visual modality (e.g., RGB or point cloud), limiting their accuracy and generalization potential. Although training a generalized DP capable of handling heterogeneous multimodal data would enhance performance, it entails substantial computational and data-related costs. To address these challenges, we propose a novel policy composition method: by leveraging multiple pre-trained DPs based on individual visual modalities, we can combine their distributional scores to form a more expressive Modality-Composable Diffusion Policy (MCDP), without the need for additional training. Through extensive empirical experiments on the RoboTwin dataset, we demonstrate the potential of MCDP to improve both adaptability and performance. This exploration aims to provide valuable insights into the flexible composition of existing DPs, facilitating the development of generalizable cross-modality, cross-domain, and even cross-embodiment policies.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">MCDP</span><span class="tag">Multimodal Diffusion Policy</span><span class="tag">Policy Composition</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.sciencedirect.com/science/article/pii/S0278612525001657" target="_blank" rel="noopener">
        <img src="../img/tab_img/PRISM_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.sciencedirect.com/science/article/pii/S0278612525001657" target="_blank" rel="noopener">Multi-robot collaborative manufacturing driven by digital twins: Advancements, challenges, and future directions</a>
        </h4>
        <p class="cite-brief">Multi-robot systems envisioned for future factories will promote advancements and capabilities of handling complex tasks and realising optimal robotic operations. However, existing multi-robot systems face challenges such as integration complexity, difficult coordination and control, low scalability, and flexibility, and thus are far from realising adaptive and efficient multi-robot collaborative manufacturing (MRCM). Digital twin technology improves visualisation, consistency, and spatial–temporal collaboration in MRCM through real-time interaction and iterative optimisation in physical and virtual spaces. Despite these improvements, barriers such as undeveloped modelling capabilities, indeterminate collaborative strategies, and limited applicability impede widespread integration of MRCM. In response to these needs, this study provides a comprehensive review of the foundational concepts, systematic architecture, and enabling technologies of digital twin-driven MRCM, serving as a prospective vision for future work in collaborative intelligent manufacturing. With the development of sensors and computational capabilities, robot intelligence is evolving towards multi-robot collaboration, including perceptual, cognitive, and behavioural collaboration. Digital twins play a critical supporting role in multi-robot collaboration, and the architecture, methodologies, and applications are elaborated across diverse stages of MRCM processes. This paper also identifies current challenges and future research directions. It encourages academic and industrial stakeholders to integrate state-of-the-art AI technologies more thoroughly into multi-robot digital twin systems for enhanced efficiency and reliability in production.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Multi-Robot Collaborative Manufacturing</span><span class="tag">Digital Twin</span><span class="tag">Intelligent Manufacturing</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2503.07511" target="_blank" rel="noopener">
        <img src="../img/tab_img/PointVLA_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2503.07511" target="_blank" rel="noopener">PointVLA: Injecting the 3D World into Vision-Language-Action Models</a>
        </h4>
        <p class="cite-brief">Vision-Language-Action (VLA) models excel at robotic tasks by leveraging large-scale 2D vision-language pretraining, but their reliance on RGB images limits spatial reasoning critical for real-world interaction. Retraining these models with 3D data is computationally prohibitive, while discarding existing 2D datasets wastes valuable resources. To bridge this gap, we propose PointVLA, a framework that enhances pre-trained VLAs with point cloud inputs without requiring retraining. Our method freezes the vanilla action expert and injects 3D features via a lightweight modular block. To identify the most effective way of integrating point cloud representations, we conduct a skip-block analysis to pinpoint less useful blocks in the vanilla action expert, ensuring that 3D features are injected only into these blocks--minimizing disruption to pre-trained representations.
Extensive experiments demonstrate that PointVLA outperforms state-of-the-art 2D imitation learning methods, such as OpenVLA, Diffusion Policy and DexVLA, across both simulated and real-world robotic tasks. Specifically, we highlight several key advantages of PointVLA enabled by point cloud integration: (1) Few-shot multi-tasking, where PointVLA successfully performs four different tasks using only 20 demonstrations each; (2) Real-vs-photo discrimination, where PointVLA distinguishes real objects from their images, leveraging 3D world knowledge to improve safety and reliability; (3) Height adaptability, Unlike conventional 2D imitation learning methods, PointVLA enables robots to adapt to objects at varying table height that unseen in train data. Furthermore, PointVLA achieves strong performance in long-horizon tasks, such as picking and packing objects from a moving conveyor belt, showcasing its ability to generalize across complex, dynamic environments.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">PointVLA</span><span class="tag">Vision-Language-Action Models</span><span class="tag">Point Cloud Integration</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.04633" target="_blank" rel="noopener">
        <img src="../img/tab_img/ReBot_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.04633" target="_blank" rel="noopener">PRISM: Pointcloud Reintegrated Inference via Segmentation and Cross-attention for Manipulation</a>
        </h4>
        <p class="cite-brief">Robust imitation learning for robot manipulation requires comprehensive 3D perception, yet many existing methods struggle in cluttered environments. Fixed camera view approaches are vulnerable to perspective changes, and 3D point cloud techniques often limit themselves to keyframes predictions, reducing their efficacy in dynamic, contact-intensive tasks. To address these challenges, we propose PRISM, designed as an end-to-end framework that directly learns from raw point cloud observations and robot states, eliminating the need for pretrained models or external datasets. PRISM comprises three main components: a segmentation embedding unit that partitions the raw point cloud into distinct object clusters and encodes local geometric details; a cross-attention component that merges these visual features with processed robot joint states to highlight relevant targets; and a diffusion module that translates the fused representation into smooth robot actions. With training on 100 demonstrations per task, PRISM surpasses both 2D and 3D baseline policies in accuracy and efficiency within our simulated environments, demonstrating strong robustness in complex, object-dense scenarios.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">PRISM</span><span class="tag">3D Point Cloud Perception</span><span class="tag">Imitation Learning</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2503.14526" target="_blank" rel="noopener">
        <img src="../img/tab_img/Recipe-for-Vision-Language-Action-Models-in-Robotic-Manipulation_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2503.14526" target="_blank" rel="noopener">ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis</a>
        </h4>
        <p class="cite-brief">Vision-language-action (VLA) models present a promising paradigm by training policies directly on real robot datasets like Open X-Embodiment. However, the high cost of real-world data collection hinders further data scaling, thereby restricting the generalizability of VLAs. In this paper, we introduce ReBot, a novel real-to-sim-to-real approach for scaling real robot datasets and adapting VLA models to target domains, which is the last-mile deployment challenge in robot manipulation. Specifically, ReBot replays real-world robot trajectories in simulation to diversify manipulated objects (real-to-sim), and integrates the simulated movements with inpainted real-world background to synthesize physically realistic and temporally consistent robot videos (sim-to-real). Our approach has several advantages: 1) it enjoys the benefit of real data to minimize the sim-to-real gap; 2) it leverages the scalability of simulation; and 3) it can generalize a pretrained VLA to a target domain with fully automated data pipelines. Extensive experiments in both simulation and real-world environments show that ReBot significantly enhances the performance and robustness of VLAs. For example, in SimplerEnv with the WidowX robot, ReBot improved the in-domain performance of Octo by 7.2% and OpenVLA by 21.8%, and out-of-domain generalization by 19.9% and 9.4%, respectively. For real-world evaluation with a Franka robot, ReBot increased the success rates of Octo by 17% and OpenVLA by 20%. More information can be found at: this https URL</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">ReBot</span><span class="tag">Vision-Language-Action Models</span><span class="tag">Real-to-Sim-to-Real</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.175624610.06665789" target="_blank" rel="noopener">
        <img src="../img/tab_img/Rethinking-Bimanual-Robotic-Manipulation_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.175624610.06665789" target="_blank" rel="noopener">Recipe for Vision-Language-Action Models in Robotic Manipulation: A Survey</a>
        </h4>
        <p class="cite-brief">This survey provides an in-depth analysis of recent advancements in foundational models that leverage large and diverse datasets to enable flexible and general-purpose manipulation skills. We reviewed the main concepts of Vision-Language-Action (VLA) models with emphasis on their connection to broader trends in robot learning and the architectural strategies that facilitate integration across vision, language, and action. The survey contains current training methodologies, dataset construction techniques, and the role of high-quality demonstrations in achieving reliable performance across diverse tasks. Key trends are identified, including the adoption of open-source tools, scaling of model size and training data, and improved generalization across platforms. In addition, we noted that VLA models demonstrate strong performance in controlled environments; however, significant challenges persist in long-term task planning, failure recovery, and operation in complex real-world scenarios. This survey aims to serve as a comprehensive reference for researchers and outline future research directions that may advance the practical deployment of VLA models in robotic manipulation.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Vision-Language-Action Models</span><span class="tag">Robotic Manipulation</span><span class="tag">Foundational Models Survey</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2503.09186" target="_blank" rel="noopener">
        <img src="../img/tab_img/RoboFactory_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2503.09186" target="_blank" rel="noopener">Rethinking Bimanual Robotic Manipulation: Learning with Decoupled Interaction Framework</a>
        </h4>
        <p class="cite-brief">Bimanual robotic manipulation is an emerging and critical topic in the robotics community. Previous works primarily rely on integrated control models that take the perceptions and states of both arms as inputs to directly predict their actions. However, we think bimanual manipulation involves not only coordinated tasks but also various uncoordinated tasks that do not require explicit cooperation during execution, such as grasping objects with the closest hand, which integrated control frameworks ignore to consider due to their enforced cooperation in the early inputs. In this paper, we propose a novel decoupled interaction framework that considers the characteristics of different tasks in bimanual manipulation. The key insight of our framework is to assign an independent model to each arm to enhance the learning of uncoordinated tasks, while introducing a selective interaction module that adaptively learns weights from its own arm to improve the learning of coordinated tasks. Extensive experiments on seven tasks in the RoboTwin dataset demonstrate that: (1) Our framework achieves outstanding performance, with a 23.5% boost over the SOTA method. (2) Our framework is flexible and can be seamlessly integrated into existing methods. (3) Our framework can be effectively extended to multi-agent manipulation tasks, achieving a 28% boost over the integrated control SOTA. (4) The performance boost stems from the decoupled design itself, surpassing the SOTA by 16.5% in success rate with only 1/6 of the model size.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Decoupled Interaction Framework</span><span class="tag">Bimanual Robotic Manipulation</span><span class="tag">Selective Interaction Module</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2503.16408" target="_blank" rel="noopener">
        <img src="../img/tab_img/RoboTransfer_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2503.16408" target="_blank" rel="noopener">RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints</a>
        </h4>
        <p class="cite-brief">Designing effective embodied multi-agent systems is critical for solving complex real-world tasks across domains. Due to the complexity of multi-agent embodied systems, existing methods fail to automatically generate safe and efficient training data for such systems. To this end, we propose the concept of compositional constraints for embodied multi-agent systems, addressing the challenges arising from collaboration among embodied agents. We design various interfaces tailored to different types of constraints, enabling seamless interaction with the physical world. Leveraging compositional constraints and specifically designed interfaces, we develop an automated data collection framework for embodied multi-agent systems and introduce the first benchmark for embodied multi-agent manipulation, RoboFactory. Based on RoboFactory benchmark, we adapt and evaluate the method of imitation learning and analyzed its performance in different difficulty agent tasks. Furthermore, we explore the architectures and training strategies for multi-agent imitation learning, aiming to build safe and efficient embodied multi-agent systems.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Compositional Constraints</span><span class="tag">Embodied Multi-Agent Systems</span><span class="tag">RoboFactory Benchmark</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2505.23171" target="_blank" rel="noopener">
        <img src="../img/tab_img/RoboTwin_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2505.23171" target="_blank" rel="noopener">RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer</a>
        </h4>
        <p class="cite-brief">Imitation Learning has become a fundamental approach in robotic manipulation. However, collecting large-scale real-world robot demonstrations is prohibitively expensive. Simulators offer a cost-effective alternative, but the sim-to-real gap make it extremely challenging to scale. Therefore, we introduce RoboTransfer, a diffusion-based video generation framework for robotic data synthesis. Unlike previous methods, RoboTransfer integrates multi-view geometry with explicit control over scene components, such as background and object attributes. By incorporating cross-view feature interactions and global depth/normal conditions, RoboTransfer ensures geometry consistency across views. This framework allows fine-grained control, including background edits and object swaps. Experiments demonstrate that RoboTransfer is capable of generating multi-view videos with enhanced geometric consistency and visual fidelity. In addition, policies trained on the data generated by RoboTransfer achieve a 33.3% relative improvement in the success rate in the DIFF-OBJ setting and a substantial 251% relative improvement in the more challenging DIFF-ALL scenario. </p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">RoboTransfer</span><span class="tag">Diffusion-Based Video Generation</span><span class="tag">Sim-to-Real Transfer</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://openaccess.thecvf.com/content/CVPR2025/html/Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins_CVPR_2025_paper.html" target="_blank" rel="noopener">
        <img src="../img/tab_img/SEM_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins_CVPR_2025_paper.html" target="_blank" rel="noopener">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</a>
        </h4>
        <p class="cite-brief">In the rapidly advancing field of robotics, dual-arm coordination and complex object manipulation are essential capabilities for developing advanced autonomous systems. However, the scarcity of diverse, high-quality demonstration data and real-world-aligned evaluation benchmarks severely limits such development. To address this, we introduce RoboTwin, a generative digital twin framework that uses 3D generative foundation models and large language models to produce diverse expert datasets and provide a real-world-aligned evaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates varied digital twins of objects from single 2D images, generating realistic and interactive scenarios. It also introduces a spatial relation-aware code generation framework that combines object annotations with large language models to break down tasks, determine spatial constraints, and generate precise robotic movement code. Our framework offers a comprehensive benchmark with both simulated and real-world data, enabling standardized evaluation and better alignment between simulated training and real-world performance. We validated our approach using the open-source COBOT Magic Robot platform. Policies pre-trained on RoboTwin-generated data and fine-tuned with limited real-world samples demonstrate significant potential for enhancing dual-arm robotic manipulation systems by improving success rates by over 70% for single-arm tasks and over 40% for dual-arm tasks compared to models trained solely on real-world data.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Embodied AI</span><span class="tag">Dual-Arm</span><span class="tag">RoboTwin</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2508.18268" target="_blank" rel="noopener">
        <img src="../img/tab_img/SafeBimanual_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2508.18268" target="_blank" rel="noopener">SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation</a>
        </h4>
        <p class="cite-brief">Bimanual manipulation has been widely applied in household services and manufacturing, which enables the complex task completion with coordination requirements. Recent diffusion-based policy learning approaches have achieved promising performance in modeling action distributions for bimanual manipulation. However, they ignored the physical safety constraints of bimanual manipulation, which leads to the dangerous behaviors with damage to robots and objects. To this end, we propose a test-time trajectory optimization framework named SafeBimanual for any pre-trained diffusion-based bimanual manipulation policies, which imposes the safety constraints on bimanual actions to avoid dangerous robot behaviors with improved success rate. Specifically, we design diverse cost functions for safety constraints in different dual-arm cooperation patterns including avoidance of tearing objects and collision between arms and objects, which optimizes the manipulator trajectories with guided sampling of diffusion denoising process. Moreover, we employ a vision-language model (VLM) to schedule the cost functions by specifying keypoints and corresponding pairwise relationship, so that the optimal safety constraint is dynamically generated in the entire bimanual manipulation process. SafeBimanual demonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase in success rate and a 18.8% reduction in unsafe interactions over state-of-the-art diffusion-based methods. Extensive experiments on 4 real-world tasks further verify its practical value by improving the success rate by 32.5%.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">SafeBimanual</span><span class="tag">Bimanual Manipulation</span><span class="tag">Safety-Constrained Diffusion Policy</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2505.16196" target="_blank" rel="noopener">
        <img src="../img/tab_img/SimLauncher_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2505.16196" target="_blank" rel="noopener">SEM: Enhancing Spatial Understanding for Robust Robot Manipulation</a>
        </h4>
        <p class="cite-brief">A key challenge in robot manipulation lies in developing policy models with strong spatial understanding, the ability to reason about 3D geometry, object relations, and robot embodiment. Existing methods often fall short: 3D point cloud models lack semantic abstraction, while 2D image encoders struggle with spatial reasoning. To address this, we propose SEM (Spatial Enhanced Manipulation model), a novel diffusion-based policy framework that explicitly enhances spatial understanding from two complementary perspectives. A spatial enhancer augments visual representations with 3D geometric context, while a robot state encoder captures embodiment-aware structure through graphbased modeling of joint dependencies. By integrating these modules, SEM significantly improves spatial understanding, leading to robust and generalizable manipulation across diverse tasks that outperform existing baselines.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">SEM</span><span class="tag">Spatial Understanding</span><span class="tag">Diffusion-Based Policy</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.04452" target="_blank" rel="noopener">
        <img src="../img/tab_img/Survey-of-Vision-Language-Action-Models_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.04452" target="_blank" rel="noopener">SimLauncher: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training</a>
        </h4>
        <p class="cite-brief">Autonomous learning of dexterous, long-horizon robotic skills has been a longstanding pursuit of embodied AI. Recent advances in robotic reinforcement learning (RL) have demonstrated remarkable performance and robustness in real-world visuomotor control tasks. However, applying RL in the real world faces challenges such as low sample efficiency, slow exploration, and significant reliance on human intervention. In contrast, simulators offer a safe and efficient environment for extensive exploration and data collection, while the visual sim-to-real gap, often a limiting factor, can be mitigated using real-to-sim techniques. Building on these, we propose SimLauncher, a novel framework that combines the strengths of real-world RL and real-to-sim-to-real approaches to overcome these challenges. Specifically, we first pre-train a visuomotor policy in the digital twin simulation environment, which then benefits real-world RL in two ways: (1) bootstrapping target values using extensive simulated demonstrations and real-world demonstrations derived from pre-trained policy rollouts, and (2) Incorporating action proposals from the pre-trained policy for better exploration. We conduct comprehensive experiments across multi-stage, contact-rich, and dexterous hand manipulation tasks. Compared to prior real-world RL approaches, SimLauncher significantly improves sample efficiency and achieves near-perfect success rates. We hope this work serves as a proof of concept and inspires further research on leveraging large-scale simulation pre-training to benefit real-world robotic RL.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">SimLauncher</span><span class="tag">Real-to-Sim-to-Real</span><span class="tag">Robotic Reinforcement Learning</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2508.15201" target="_blank" rel="noopener">
        <img src="../img/tab_img/TypeTele_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2508.15201" target="_blank" rel="noopener">Survey of Vision-Language-Action Models for Embodied Manipulation</a>
        </h4>
        <p class="cite-brief">Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Embodied Intelligence</span><span class="tag">VLA Models</span><span class="tag">Robotic Manipulation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2507.01857" target="_blank" rel="noopener">
        <img src="../img/tab_img/Vision-Based-Contact-Force-Sensing-in-Robotic_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2507.01857" target="_blank" rel="noopener">TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types</a>
        </h4>
        <p class="cite-brief">Dexterous teleoperation plays a crucial role in robotic manipulation for real-world data collection and remote robot control. Previous dexterous teleoperation mostly relies on hand retargeting to closely mimic human hand postures. However, these approaches may fail to fully leverage the inherent dexterity of dexterous hands, which can execute unique actions through their structural advantages compared to human hands. To address this limitation, we propose TypeTele, a type-guided dexterous teleoperation system, which enables dexterous hands to perform actions that are not constrained by human motion patterns. This is achieved by introducing dexterous manipulation types into the teleoperation system, allowing operators to employ appropriate types to complete specific tasks. To support this system, we build an extensible dexterous manipulation type library to cover comprehensive dexterous postures used in manipulation tasks. During teleoperation, we employ a MLLM (Multi-modality Large Language Model)-assisted type retrieval module to identify the most suitable manipulation type based on the specific task and operator commands. Extensive experiments of real-world teleoperation and imitation learning demonstrate that the incorporation of manipulation types significantly takes full advantage of the dexterous robot's ability to perform diverse and complex tasks with higher success rates.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">TypeTele</span><span class="tag">Dexterous Teleoperation</span><span class="tag">Manipulation Types</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://ieeexplore.ieee.org/abstract/document/11051000/" target="_blank" rel="noopener">
        <img src="../img/tab_img/Vision-Language-Action-Models-for-Robotics_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://ieeexplore.ieee.org/abstract/document/11051000/" target="_blank" rel="noopener">Vision-Based Contact Force Sensing in Robotic Surgery: A Technical Review</a>
        </h4>
        <p class="cite-brief">Contact force sensing plays a fundamental role in enabling precise and safe manipulations in surgical robotics. While traditional force sensing methods predominantly rely on physical sensors, they encounter significant limitations including integration complexity, limited durability, and scalability constraints in surgical environments. This paper presents a comprehensive survey of vision-based force sensing approaches that leverage computer vision and image processing techniques to infer contact forces without dedicated physical sensors. The survey systematically analyzes state-of-the-art methodologies encompassing vision-based force estimation, surface deformation analysis, and machine learning-driven perception frameworks. The underlying principles, algorithmic architectures, and surgical applications of these approaches are examined in detail. Critical challenges including real-time processing requirements, robustness to environmental variations, and cross-task generalizability are thoroughly investigated. The survey highlights the distinctive advantages of purely vision-based approaches, including reduced hardware complexity, enhanced adaptability, and seamless integration potential with existing surgical robotic platforms. Furthermore, this work identifies critical open research questions and promising future directions for advancing vision-based force sensing technologies in next-generation surgical robotics. This comprehensive review serves as an authoritative reference for researchers and practitioners working to overcome the limitations of traditional contact force sensing through vision-based solutions in surgical applications.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Vision-Based Force Sensing</span><span class="tag">Surgical Robotics</span><span class="tag">Contact Force Estimation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.175502755.53627529" target="_blank" rel="noopener">
        <img src="../img/tab_img/WorldEval_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://www.techrxiv.org/doi/full/10.36227/techrxiv.175502755.53627529" target="_blank" rel="noopener">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a>
        </h4>
        <p class="cite-brief">Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable realworld deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">VLA Models</span><span class="tag">Robotics Generalisation</span><span class="tag">Full-Stack Systems</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2505.19017" target="_blank" rel="noopener">
        <img src="../img/tab_img/You-Only-Teach-Once_img.jpg" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2505.19017" target="_blank" rel="noopener">WorldEval: World Model as Real-World Robot Policies Evaluator</a>
        </h4>
        <p class="cite-brief">The field of robotics has made significant strides toward developing generalist robot manipulation policies. However, evaluating these policies in real-world scenarios remains time-consuming and challenging, particularly as the number of tasks scales and environmental conditions change. In this work, we demonstrate that world models can serve as a scalable, reproducible, and reliable proxy for real-world robot policy evaluation. A key challenge is generating accurate policy videos from world models that faithfully reflect the robot actions. We observe that directly inputting robot actions or using high-dimensional encoding methods often fails to generate action-following videos. To address this, we propose Policy2Vec, a simple yet effective approach to turn a video generation model into a world simulator that follows latent action to generate the robot video. We then introduce WorldEval, an automated pipeline designed to evaluate real-world robot policies entirely online. WorldEval effectively ranks various robot policies and individual checkpoints within a single policy, and functions as a safety detector to prevent dangerous actions by newly developed robot models. Through comprehensive paired evaluations of manipulation policies in real-world environments, we demonstrate a strong correlation between policy performance in WorldEval and real-world scenarios. Furthermore, our method significantly outperforms popular methods such as real-to-sim approach.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">Policy2Vec</span><span class="tag">WorldEval</span><span class="tag">Robot Policy Evaluation</span>
        </div>
      </div>
    </article>
    
    <article class="cite-card">
      <a class="thumb" href="https://arxiv.org/abs/2501.14208" target="_blank" rel="noopener">
        <img src="nan" alt="cover image" />
      </a>
      <div class="content">
        <h4 class="cite-title">
          <a href="https://arxiv.org/abs/2501.14208" target="_blank" rel="noopener">You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations</a>
        </h4>
        <p class="cite-brief">Bimanual robotic manipulation is a long-standing challenge of embodied intelligence due to its characteristics of dual-arm spatial-temporal coordination and high-dimensional action spaces. Previous studies rely on pre-defined action taxonomies or direct teleoperation to alleviate or circumvent these issues, often making them lack simplicity, versatility and scalability. Differently, we believe that the most effective and efficient way for teaching bimanual manipulation is learning from human demonstrated videos, where rich features such as spatial-temporal positions, dynamic postures, interaction states and dexterous transitions are available almost for free. In this work, we propose the YOTO (You Only Teach Once), which can extract and then inject patterns of bimanual actions from as few as a single binocular observation of hand movements, and teach dual robot arms various complex tasks. Furthermore, based on keyframes-based motion trajectories, we devise a subtle solution for rapidly generating training demonstrations with diverse variations of manipulated objects and their locations. These data can then be used to learn a customized bimanual diffusion policy (BiDP) across diverse scenes. In experiments, YOTO achieves impressive performance in mimicking 5 intricate long-horizon bimanual tasks, possesses strong generalization under different visual and spatial conditions, and outperforms existing visuomotor imitation learning methods in accuracy and efficiency. Our project link is this https URL.</p>
        <div class="tagbar" aria-label="tags">
          <span class="tag">YOTO</span><span class="tag">Bimanual Robotic Manipulation</span><span class="tag">Diffusion Policy</span>
        </div>
      </div>
    </article>
    
  </section>
</body>
</html>
